{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning Analytics (Ben)\n",
        "\n",
        "This notebook focuses on user **`ben`** and the metrics we discussed:\n",
        "\n",
        "- Words studied (unique, cumulative over time)\n",
        "- Words learned (`retrievability >= R_TARGET`, cumulative-ever over time)\n",
        "- Verb conjugation learned (point-in-time over time; remembered only when both perfectum and past tense are `>= R_TARGET`)\n",
        "- Study time from **session span** (`max(timestamp) - min(timestamp)` per session, summed per day)\n",
        "\n",
        "All dates are bucketed in **UTC**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "try:\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "except ModuleNotFoundError:\n",
        "    get_ipython().run_line_magic('pip', 'install -q plotly')\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "if not (ROOT / 'core').exists():\n",
        "    ROOT = ROOT.parent\n",
        "load_dotenv(ROOT / '.env')\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from core import fsrs\n",
        "from core.fsrs.constants import R_TARGET\n",
        "from core.fsrs.database import get_session\n",
        "from core.fsrs.models import ReviewEvent\n",
        "\n",
        "USER_ID = 'ben'\n",
        "WORD_EX = 'word_translation'\n",
        "VERB_EX = ['verb_perfectum', 'verb_past_tense']\n",
        "pd.set_option('display.max_rows', 200)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session = get_session()\n",
        "try:\n",
        "    events = pd.read_sql(\n",
        "        session.query(ReviewEvent).filter(ReviewEvent.user_id == USER_ID).statement,\n",
        "        session.bind,\n",
        "    )\n",
        "finally:\n",
        "    session.close()\n",
        "\n",
        "if events.empty:\n",
        "    raise ValueError(f'No review events found for user_id={USER_ID!r}.')\n",
        "\n",
        "events['timestamp'] = pd.to_datetime(events['timestamp'], utc=True)\n",
        "events = events.sort_values('timestamp').reset_index(drop=True)\n",
        "events['event_day_utc'] = events['timestamp'].dt.floor('D')\n",
        "\n",
        "start_day = events['event_day_utc'].min()\n",
        "end_day = pd.Timestamp.now(tz='UTC').floor('D')\n",
        "all_days = pd.date_range(start_day, end_day, freq='D', tz='UTC')\n",
        "\n",
        "print(f'Loaded {len(events):,} events for user {USER_ID!r}')\n",
        "print(f'Date range (UTC): {start_day.date()} -> {end_day.date()}')\n",
        "display(events.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cumulative_from_first_dates(first_dates: pd.Series, day_index: pd.DatetimeIndex) -> pd.Series:\n",
        "    if first_dates.empty:\n",
        "        return pd.Series(0, index=day_index, dtype='int64')\n",
        "    counts = first_dates.value_counts().sort_index()\n",
        "    return counts.reindex(day_index, fill_value=0).cumsum().astype('int64')\n",
        "\n",
        "def daily_session_span_hours(events_df: pd.DataFrame, exercise_types: list[str], day_index: pd.DatetimeIndex) -> pd.Series:\n",
        "    scoped = events_df[\n",
        "        events_df['exercise_type'].isin(exercise_types)\n",
        "        & events_df['session_id'].notna()\n",
        "    ].copy()\n",
        "    if scoped.empty:\n",
        "        return pd.Series(0.0, index=day_index, dtype='float64')\n",
        "\n",
        "    spans = scoped.groupby('session_id').agg(\n",
        "        session_start=('timestamp', 'min'),\n",
        "        session_end=('timestamp', 'max'),\n",
        "    )\n",
        "    spans['span_hours'] = (spans['session_end'] - spans['session_start']).dt.total_seconds() / 3600.0\n",
        "    spans['day_utc'] = spans['session_start'].dt.floor('D')\n",
        "\n",
        "    daily = spans.groupby('day_utc')['span_hours'].sum()\n",
        "    return daily.reindex(day_index, fill_value=0.0)\n",
        "\n",
        "def daily_card_retrievability(events_df: pd.DataFrame, exercise_type: str, day_index: pd.DatetimeIndex) -> pd.DataFrame:\n",
        "    scoped = events_df.loc[\n",
        "        events_df['exercise_type'] == exercise_type,\n",
        "        ['word_id', 'timestamp', 'is_ltm_event', 'stability_after']\n",
        "    ].copy()\n",
        "\n",
        "    if scoped.empty:\n",
        "        return pd.DataFrame(columns=['day', 'word_id', 'retrievability'])\n",
        "\n",
        "    scoped = scoped.sort_values(['word_id', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "    snapshots = pd.DataFrame({'day': day_index})\n",
        "    snapshots['snapshot_ts'] = snapshots['day'] + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1)\n",
        "\n",
        "    out_parts = []\n",
        "\n",
        "    for word_id, grp in scoped.groupby('word_id', sort=False):\n",
        "        g = grp.sort_values('timestamp').copy()\n",
        "        g['last_ltm_ts'] = g['timestamp'].where(g['is_ltm_event'].astype(bool)).ffill()\n",
        "\n",
        "        merged = pd.merge_asof(\n",
        "            snapshots.sort_values('snapshot_ts'),\n",
        "            g[['timestamp', 'stability_after', 'last_ltm_ts']].sort_values('timestamp'),\n",
        "            left_on='snapshot_ts',\n",
        "            right_on='timestamp',\n",
        "            direction='backward',\n",
        "        )\n",
        "\n",
        "        valid = merged['stability_after'].notna() & merged['last_ltm_ts'].notna()\n",
        "        if not valid.any():\n",
        "            continue\n",
        "\n",
        "        days_since = (merged.loc[valid, 'snapshot_ts'] - merged.loc[valid, 'last_ltm_ts']).dt.total_seconds() / 86400.0\n",
        "        retr = np.exp(-days_since / merged.loc[valid, 'stability_after'])\n",
        "\n",
        "        out_parts.append(pd.DataFrame({\n",
        "            'day': merged.loc[valid, 'day'].to_numpy(),\n",
        "            'word_id': word_id,\n",
        "            'retrievability': retr.to_numpy(),\n",
        "        }))\n",
        "\n",
        "    if not out_parts:\n",
        "        return pd.DataFrame(columns=['day', 'word_id', 'retrievability'])\n",
        "\n",
        "    return pd.concat(out_parts, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Words track ----\n",
        "word_events = events[events['exercise_type'] == WORD_EX].copy()\n",
        "\n",
        "word_first_studied = word_events.groupby('word_id')['timestamp'].min().dt.floor('D')\n",
        "word_studied_cum = cumulative_from_first_dates(word_first_studied, all_days)\n",
        "\n",
        "word_state_events = word_events[['word_id', 'timestamp', 'is_ltm_event', 'stability_after']].copy()\n",
        "word_state_events = word_state_events.sort_values(['word_id', 'timestamp'])\n",
        "word_state_events['last_ltm_ts'] = word_state_events['timestamp'].where(word_state_events['is_ltm_event'].astype(bool))\n",
        "word_state_events['last_ltm_ts'] = word_state_events.groupby('word_id')['last_ltm_ts'].ffill()\n",
        "word_state_events['days_since_ltm'] = (word_state_events['timestamp'] - word_state_events['last_ltm_ts']).dt.total_seconds() / 86400.0\n",
        "word_state_events['retr_after'] = np.exp(-word_state_events['days_since_ltm'] / word_state_events['stability_after'])\n",
        "\n",
        "word_first_learned = (\n",
        "    word_state_events[word_state_events['retr_after'] >= R_TARGET]\n",
        "    .groupby('word_id')['timestamp']\n",
        "    .min()\n",
        "    .dt.floor('D')\n",
        ")\n",
        "word_learned_cum = cumulative_from_first_dates(word_first_learned, all_days)\n",
        "\n",
        "word_snaps = fsrs.get_all_cards_with_state(WORD_EX, USER_ID)\n",
        "word_snap_df = pd.DataFrame([{'word_id': s.word_id, 'retrievability': s.retrievability} for s in word_snaps])\n",
        "\n",
        "current_words_studied = int(word_events['word_id'].nunique())\n",
        "current_words_learned = int((word_snap_df['retrievability'] >= R_TARGET).sum()) if not word_snap_df.empty else 0\n",
        "\n",
        "word_span_daily = daily_session_span_hours(events, [WORD_EX], all_days)\n",
        "word_span_cum = word_span_daily.cumsum()\n",
        "\n",
        "print('Words track KPIs')\n",
        "print('----------------')\n",
        "print(f'Current words studied (unique): {current_words_studied:,}')\n",
        "print(f'Current words learned (retrievability >= {R_TARGET:.2f}): {current_words_learned:,}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Conjugation track ----\n",
        "perf_daily = daily_card_retrievability(events, 'verb_perfectum', all_days)\n",
        "past_daily = daily_card_retrievability(events, 'verb_past_tense', all_days)\n",
        "\n",
        "conj_daily = perf_daily.merge(\n",
        "    past_daily,\n",
        "    on=['day', 'word_id'],\n",
        "    suffixes=('_perf', '_past'),\n",
        "    how='inner',\n",
        ")\n",
        "\n",
        "if conj_daily.empty:\n",
        "    conjugation_learned_point_in_time = pd.Series(0, index=all_days, dtype='int64')\n",
        "else:\n",
        "    conj_daily['remembered'] = (\n",
        "        (conj_daily['retrievability_perf'] >= R_TARGET)\n",
        "        & (conj_daily['retrievability_past'] >= R_TARGET)\n",
        "    )\n",
        "    conjugation_learned_point_in_time = (\n",
        "        conj_daily[conj_daily['remembered']]\n",
        "        .groupby('day')['word_id']\n",
        "        .nunique()\n",
        "        .reindex(all_days, fill_value=0)\n",
        "        .astype('int64')\n",
        "    )\n",
        "\n",
        "perf_snap = pd.DataFrame([{'word_id': s.word_id, 'r_perf': s.retrievability} for s in fsrs.get_all_cards_with_state('verb_perfectum', USER_ID)])\n",
        "past_snap = pd.DataFrame([{'word_id': s.word_id, 'r_past': s.retrievability} for s in fsrs.get_all_cards_with_state('verb_past_tense', USER_ID)])\n",
        "\n",
        "conj_current = perf_snap.merge(past_snap, on='word_id', how='outer').fillna(0.0)\n",
        "current_conj_learned = int(((conj_current['r_perf'] >= R_TARGET) & (conj_current['r_past'] >= R_TARGET)).sum()) if not conj_current.empty else 0\n",
        "\n",
        "conj_span_daily = daily_session_span_hours(events, VERB_EX, all_days)\n",
        "conj_span_cum = conj_span_daily.cumsum()\n",
        "\n",
        "print('Conjugation track KPIs')\n",
        "print('----------------------')\n",
        "print(f'Current conjugation learned (both tenses >= {R_TARGET:.2f}): {current_conj_learned:,}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Toggle here: 'words' or 'conjugation'\n",
        "TRACK = 'words'\n",
        "\n",
        "if TRACK not in {'words', 'conjugation'}:\n",
        "    raise ValueError(\"TRACK must be 'words' or 'conjugation'\")\n",
        "\n",
        "if TRACK == 'words':\n",
        "    fig_growth = go.Figure()\n",
        "    fig_growth.add_trace(go.Scatter(x=all_days, y=word_studied_cum, mode='lines', name='Words Studied (Cumulative)', line=dict(width=3)))\n",
        "    fig_growth.add_trace(go.Scatter(x=all_days, y=word_learned_cum, mode='lines', name='Words Learned (Cumulative Ever, >R)', line=dict(width=3)))\n",
        "    fig_growth.update_layout(\n",
        "        title='Words: Studied vs Learned Over Time (UTC)',\n",
        "        xaxis_title='Date (UTC)',\n",
        "        yaxis_title='Unique Words',\n",
        "        hovermode='x unified',\n",
        "        template='plotly_white',\n",
        "    )\n",
        "    fig_growth.show()\n",
        "\n",
        "    fig_time = make_subplots(specs=[[{'secondary_y': True}]])\n",
        "    fig_time.add_trace(go.Bar(x=all_days, y=word_span_daily, name='Daily Session Span Hours'), secondary_y=False)\n",
        "    fig_time.add_trace(go.Scatter(x=all_days, y=word_span_cum, name='Cumulative Session Span Hours', line=dict(width=3)), secondary_y=True)\n",
        "    fig_time.update_layout(\n",
        "        title='Words: Study Time from Session Span (UTC)',\n",
        "        xaxis_title='Date (UTC)',\n",
        "        hovermode='x unified',\n",
        "        template='plotly_white',\n",
        "    )\n",
        "    fig_time.update_yaxes(title_text='Daily Hours', secondary_y=False)\n",
        "    fig_time.update_yaxes(title_text='Cumulative Hours', secondary_y=True)\n",
        "    fig_time.show()\n",
        "\n",
        "else:\n",
        "    fig_conj = go.Figure()\n",
        "    fig_conj.add_trace(go.Scatter(\n",
        "        x=all_days,\n",
        "        y=conjugation_learned_point_in_time,\n",
        "        mode='lines',\n",
        "        name='Conjugation Learned (Point-in-Time)',\n",
        "        line=dict(width=3),\n",
        "    ))\n",
        "    fig_conj.update_layout(\n",
        "        title='Conjugation: Learned Over Time (Point-in-Time, UTC)',\n",
        "        xaxis_title='Date (UTC)',\n",
        "        yaxis_title='Unique Verbs Remembered (both tenses > R_TARGET)',\n",
        "        hovermode='x unified',\n",
        "        template='plotly_white',\n",
        "    )\n",
        "    fig_conj.show()\n",
        "\n",
        "    fig_time = make_subplots(specs=[[{'secondary_y': True}]])\n",
        "    fig_time.add_trace(go.Bar(x=all_days, y=conj_span_daily, name='Daily Session Span Hours'), secondary_y=False)\n",
        "    fig_time.add_trace(go.Scatter(x=all_days, y=conj_span_cum, name='Cumulative Session Span Hours', line=dict(width=3)), secondary_y=True)\n",
        "    fig_time.update_layout(\n",
        "        title='Conjugation: Study Time from Session Span (UTC)',\n",
        "        xaxis_title='Date (UTC)',\n",
        "        hovermode='x unified',\n",
        "        template='plotly_white',\n",
        "    )\n",
        "    fig_time.update_yaxes(title_text='Daily Hours', secondary_y=False)\n",
        "    fig_time.update_yaxes(title_text='Cumulative Hours', secondary_y=True)\n",
        "    fig_time.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = pd.DataFrame({\n",
        "    'day_utc': all_days,\n",
        "    'words_studied_cum': word_studied_cum.values,\n",
        "    'words_learned_cum_ever': word_learned_cum.values,\n",
        "    'conjugation_learned_point_in_time': conjugation_learned_point_in_time.values,\n",
        "    'word_session_span_hours_daily': word_span_daily.values,\n",
        "    'verb_session_span_hours_daily': conj_span_daily.values,\n",
        "})\n",
        "\n",
        "display(summary.tail(20))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}